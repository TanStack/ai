// This file is auto-generated by @hey-api/openapi-ts

export type ClientOptions = {
  baseUrl: 'https://queue.fal.run' | (string & {})
}

/**
 * VideoOutput
 */
export type SchemaRouterVideoOutput = {
  /**
   * Usage
   *
   * Token usage information
   */
  usage?: SchemaUsageInfo
  /**
   * Output
   *
   * Generated output from video processing
   */
  output: string
}

/**
 * UsageInfo
 */
export type SchemaUsageInfo = {
  /**
   * Prompt Tokens
   */
  prompt_tokens?: number
  /**
   * Total Tokens
   */
  total_tokens?: number
  /**
   * Completion Tokens
   */
  completion_tokens?: number
  /**
   * Cost
   */
  cost: number
}

/**
 * VideoInput
 */
export type SchemaRouterVideoInput = {
  /**
   * Prompt
   *
   * Prompt to be used for the video processing
   */
  prompt: string
  /**
   * Video Urls
   *
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string | Blob | File>
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number
}

/**
 * VideoOutput
 */
export type SchemaRouterVideoEnterpriseOutput = {
  /**
   * Usage
   *
   * Token usage information
   */
  usage?: SchemaUsageInfo
  /**
   * Output
   *
   * Generated output from video processing
   */
  output: string
}

/**
 * VideoEnterpriseInput
 */
export type SchemaRouterVideoEnterpriseInput = {
  /**
   * Prompt
   *
   * Prompt to be used for the video processing
   */
  prompt: string
  /**
   * Video Urls
   *
   * List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url.
   */
  video_urls?: Array<string | Blob | File>
  /**
   * System Prompt
   *
   * System prompt to provide context or instructions to the model
   */
  system_prompt?: string
  /**
   * Reasoning
   *
   * Should reasoning be the part of the final answer.
   */
  reasoning?: boolean
  /**
   * Model
   *
   * Name of the model to use. Charged based on actual token usage.
   */
  model: string
  /**
   * Max Tokens
   *
   * This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.
   */
  max_tokens?: number
  /**
   * Temperature
   *
   * This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.
   */
  temperature?: number
}

/**
 * AITextDetectionOutput
 */
export type SchemaAiDetectorDetectTextOutput = {
  /**
   * Latency
   */
  latency: number
  /**
   * Verdict
   */
  verdict: string
  /**
   * Is Ai Generated
   */
  is_ai_generated: boolean
  /**
   * Confidence
   */
  confidence: number
}

/**
 * TextDetectionInput
 */
export type SchemaAiDetectorDetectTextInput = {
  /**
   * Text
   *
   * Text content to analyze for AI generation.
   */
  text: string
}

/**
 * DiarizationSegment
 */
export type SchemaDiarizationSegment = {
  /**
   * Timestamp
   *
   * Start and end timestamp of the segment
   */
  timestamp: [unknown, unknown]
  /**
   * Speaker
   *
   * Speaker ID of the segment
   */
  speaker: string
}

/**
 * WhisperOutput
 */
export type SchemaWhisperOutput = {
  /**
   * Text
   *
   * Transcription of the audio file
   */
  text: string
  /**
   * Inferred Languages
   *
   * List of languages that the audio file is inferred to be. Defaults to null.
   */
  inferred_languages: Array<
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
  >
  /**
   * Chunks
   *
   * Timestamp chunks of the audio file
   */
  chunks?: Array<SchemaWhisperChunk>
  /**
   * Diarization Segments
   *
   * Speaker diarization segments of the audio file. Only present if diarization is enabled.
   */
  diarization_segments: Array<SchemaDiarizationSegment>
}

/**
 * WhisperChunk
 */
export type SchemaWhisperChunk = {
  /**
   * Text
   *
   * Transcription of the chunk
   */
  text: string
  /**
   * Timestamp
   *
   * Start and end timestamp of the chunk
   */
  timestamp: [unknown, unknown]
  /**
   * Speaker
   *
   * Speaker ID of the chunk. Only present if diarization is enabled.
   */
  speaker?: string
}

/**
 * WhisperInput
 */
export type SchemaWhisperInput = {
  /**
   * Version
   *
   * Version of the model to use. All of the models are the Whisper large variant.
   */
  version?: '3'
  /**
   * Batch Size
   */
  batch_size?: number
  /**
   * Language
   *
   *
   * Language of the audio file. If set to null, the language will be
   * automatically detected. Defaults to null.
   *
   * If translate is selected as the task, the audio will be translated to
   * English, regardless of the language selected.
   *
   */
  language?:
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
  /**
   * Prompt
   *
   * Prompt to use for generation. Defaults to an empty string.
   */
  prompt?: string
  /**
   * Num Speakers
   *
   *
   * Number of speakers in the audio file. Defaults to null.
   * If not provided, the number of speakers will be automatically
   * detected.
   *
   */
  num_speakers?: number | null
  /**
   * Task
   *
   * Task to perform on the audio file. Either transcribe or translate.
   */
  task?: 'transcribe' | 'translate'
  /**
   * Chunk Level
   *
   * Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**.
   */
  chunk_level?: 'none' | 'segment' | 'word'
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
   */
  audio_url: string | Blob | File
  /**
   * Diarize
   *
   * Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time.
   */
  diarize?: boolean
}

/**
 * WhisperOutput
 */
export type SchemaWizperOutput = {
  /**
   * Text
   *
   * Transcription of the audio file
   */
  text: string
  /**
   * Languages
   *
   * List of languages that the audio file is inferred to be. Defaults to null.
   */
  languages: Array<
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
  >
  /**
   * Chunks
   *
   * Timestamp chunks of the audio file
   */
  chunks: Array<SchemaWhisperChunk>
}

/**
 * WhisperInput
 */
export type SchemaWizperInput = {
  /**
   * Language
   *
   *
   * Language of the audio file.
   * If translate is selected as the task, the audio will be translated to
   * English, regardless of the language selected. If `None` is passed,
   * the language will be automatically detected. This will also increase
   * the inference time.
   *
   */
  language?:
    | 'af'
    | 'am'
    | 'ar'
    | 'as'
    | 'az'
    | 'ba'
    | 'be'
    | 'bg'
    | 'bn'
    | 'bo'
    | 'br'
    | 'bs'
    | 'ca'
    | 'cs'
    | 'cy'
    | 'da'
    | 'de'
    | 'el'
    | 'en'
    | 'es'
    | 'et'
    | 'eu'
    | 'fa'
    | 'fi'
    | 'fo'
    | 'fr'
    | 'gl'
    | 'gu'
    | 'ha'
    | 'haw'
    | 'he'
    | 'hi'
    | 'hr'
    | 'ht'
    | 'hu'
    | 'hy'
    | 'id'
    | 'is'
    | 'it'
    | 'ja'
    | 'jw'
    | 'ka'
    | 'kk'
    | 'km'
    | 'kn'
    | 'ko'
    | 'la'
    | 'lb'
    | 'ln'
    | 'lo'
    | 'lt'
    | 'lv'
    | 'mg'
    | 'mi'
    | 'mk'
    | 'ml'
    | 'mn'
    | 'mr'
    | 'ms'
    | 'mt'
    | 'my'
    | 'ne'
    | 'nl'
    | 'nn'
    | 'no'
    | 'oc'
    | 'pa'
    | 'pl'
    | 'ps'
    | 'pt'
    | 'ro'
    | 'ru'
    | 'sa'
    | 'sd'
    | 'si'
    | 'sk'
    | 'sl'
    | 'sn'
    | 'so'
    | 'sq'
    | 'sr'
    | 'su'
    | 'sv'
    | 'sw'
    | 'ta'
    | 'te'
    | 'tg'
    | 'th'
    | 'tk'
    | 'tl'
    | 'tr'
    | 'tt'
    | 'uk'
    | 'ur'
    | 'uz'
    | 'vi'
    | 'yi'
    | 'yo'
    | 'zh'
    | unknown
  /**
   * Version
   *
   * Version of the model to use. All of the models are the Whisper large variant.
   */
  version?: string
  /**
   * Max Segment Len
   *
   * Maximum speech segment duration in seconds before splitting.
   */
  max_segment_len?: number
  /**
   * Task
   *
   * Task to perform on the audio file. Either transcribe or translate.
   */
  task?: 'transcribe' | 'translate'
  /**
   * Chunk Level
   *
   * Level of the chunks to return.
   */
  chunk_level?: string
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm.
   */
  audio_url: string | Blob | File
  /**
   * Merge Chunks
   *
   * Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len.
   */
  merge_chunks?: boolean
}

/**
 * TranscriptionOutput
 */
export type SchemaElevenlabsSpeechToTextOutput = {
  /**
   * Text
   *
   * The full transcribed text
   */
  text: string
  /**
   * Language Probability
   *
   * Confidence in language detection
   */
  language_probability: number
  /**
   * Language Code
   *
   * Detected or specified language code
   */
  language_code: string
  /**
   * Words
   *
   * Word-level transcription details
   */
  words: Array<SchemaTranscriptionWord>
}

/**
 * TranscriptionWord
 */
export type SchemaTranscriptionWord = {
  /**
   * Text
   *
   * The transcribed word or audio event
   */
  text: string
  /**
   * Start
   *
   * Start time in seconds
   */
  start: number | unknown
  /**
   * Type
   *
   * Type of element (word, spacing, or audio_event)
   */
  type: string
  /**
   * End
   *
   * End time in seconds
   */
  end: number | unknown
  /**
   * Speaker Id
   *
   * Speaker identifier if diarization was enabled
   */
  speaker_id?: string | unknown
}

/**
 * SpeechToTextRequest
 */
export type SchemaElevenlabsSpeechToTextInput = {
  /**
   * Language Code
   *
   * Language code of the audio
   */
  language_code?: string | unknown
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe
   */
  audio_url: string | Blob | File
  /**
   * Diarize
   *
   * Whether to annotate who is speaking
   */
  diarize?: boolean
  /**
   * Tag Audio Events
   *
   * Tag audio events like laughter, applause, etc.
   */
  tag_audio_events?: boolean
}

/**
 * SpeechOutput
 */
export type SchemaSpeechToTextOutput = {
  /**
   * Partial
   *
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean
  /**
   * Transcribed Text
   *
   * The partial or final transcription output from Canary
   */
  output: string
}

/**
 * SpeechInput
 */
export type SchemaSpeechToTextInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

export type SchemaSpeechToTextStreamOutput = unknown

/**
 * SpeechInput
 */
export type SchemaSpeechToTextStreamInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

export type SchemaSpeechToTextTurboStreamOutput = unknown

/**
 * SpeechInput
 */
export type SchemaSpeechToTextTurboStreamInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

/**
 * SpeechOutput
 */
export type SchemaSpeechToTextTurboOutput = {
  /**
   * Partial
   *
   * Indicates if this is a partial (in-progress) transcript
   */
  partial?: boolean
  /**
   * Transcribed Text
   *
   * The partial or final transcription output from Canary
   */
  output: string
}

/**
 * SpeechInput
 */
export type SchemaSpeechToTextTurboInput = {
  /**
   * Audio Path
   *
   * Local filesystem path (or remote URL) to a long audio file
   */
  audio_url: string | Blob | File
  /**
   * Use Punctuation/Capitalization (PnC)
   *
   * Whether to use Canary's built-in punctuation & capitalization
   */
  use_pnc?: boolean
}

/**
 * Output
 */
export type SchemaSmartTurnOutput = {
  /**
   * Prediction
   *
   * The predicted turn type. 1 for Complete, 0 for Incomplete.
   */
  prediction: number
  /**
   * Probability
   *
   * The probability of the predicted turn type.
   */
  probability: number
  /**
   * Metrics
   *
   * The metrics of the inference.
   */
  metrics: {
    [key: string]: unknown
  }
}

/**
 * SmartTurnInput
 */
export type SchemaSmartTurnInput = {
  /**
   * Audio Url
   *
   * The URL of the audio file to be processed.
   */
  audio_url: string | Blob | File
}

/**
 * TranscriptionOutputV2
 */
export type SchemaElevenlabsSpeechToTextScribeV2Output = {
  /**
   * Text
   *
   * The full transcribed text
   */
  text: string
  /**
   * Language Probability
   *
   * Confidence in language detection
   */
  language_probability: number
  /**
   * Language Code
   *
   * Detected or specified language code
   */
  language_code: string
  /**
   * Words
   *
   * Word-level transcription details
   */
  words: Array<SchemaTranscriptionWord>
}

/**
 * SpeechToTextRequestScribeV2
 */
export type SchemaElevenlabsSpeechToTextScribeV2Input = {
  /**
   * Keyterms
   *
   * Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price.
   */
  keyterms?: Array<string>
  /**
   * Audio Url
   *
   * URL of the audio file to transcribe
   */
  audio_url: string | Blob | File
  /**
   * Diarize
   *
   * Whether to annotate who is speaking
   */
  diarize?: boolean
  /**
   * Language Code
   *
   * Language code of the audio
   */
  language_code?: string | unknown
  /**
   * Tag Audio Events
   *
   * Tag audio events like laughter, applause, etc.
   */
  tag_audio_events?: boolean
}

/**
 * SpeechTimestamp
 */
export type SchemaSpeechTimestamp = {
  /**
   * End Time
   *
   * The end time of the speech in seconds.
   */
  end: number
  /**
   * Start Time
   *
   * The start time of the speech in seconds.
   */
  start: number
}

/**
 * SileroVADOutput
 */
export type SchemaSileroVadOutput = {
  /**
   * Has Speech
   *
   * Whether the audio has speech.
   */
  has_speech: boolean
  /**
   * Speech Timestamps
   *
   * The speech timestamps.
   */
  timestamps: Array<SchemaSpeechTimestamp>
}

/**
 * SileroVADInput
 */
export type SchemaSileroVadInput = {
  /**
   * Audio URL
   *
   * The URL of the audio to get speech timestamps from.
   */
  audio_url: string | Blob | File
}

/**
 * SpeechOutput
 */
export type SchemaNemotronAsrOutput = {
  /**
   * Partial Result
   *
   * True if this is an intermediate result during streaming.
   */
  partial?: boolean
  /**
   * Transcribed Text
   *
   * The transcribed text from the audio.
   */
  output: string
}

/**
 * SpeechInput
 */
export type SchemaNemotronAsrInput = {
  /**
   * Acceleration
   *
   * Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER).
   */
  acceleration?: 'none' | 'low' | 'medium' | 'high'
  /**
   * Audio URL
   *
   * URL of the audio file.
   */
  audio_url: string | Blob | File
}

export type SchemaNemotronAsrStreamOutput = unknown

/**
 * SpeechInput
 */
export type SchemaNemotronAsrStreamInput = {
  /**
   * Acceleration
   *
   * Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER).
   */
  acceleration?: 'none' | 'low' | 'medium' | 'high'
  /**
   * Audio URL
   *
   * URL of the audio file.
   */
  audio_url: string | Blob | File
}

export type SchemaQueueStatus = {
  status: 'IN_QUEUE' | 'IN_PROGRESS' | 'COMPLETED'
  /**
   * The request id.
   */
  request_id: string
  /**
   * The response url.
   */
  response_url?: string
  /**
   * The status url.
   */
  status_url?: string
  /**
   * The cancel url.
   */
  cancel_url?: string
  /**
   * The logs.
   */
  logs?: {
    [key: string]: unknown
  }
  /**
   * The metrics.
   */
  metrics?: {
    [key: string]: unknown
  }
  /**
   * The queue position.
   */
  queue_position?: number
}

export type GetFalAiNemotronAsrStreamRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/nemotron/asr/stream/requests/{request_id}/status'
}

export type GetFalAiNemotronAsrStreamRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiNemotronAsrStreamRequestsByRequestIdStatusResponse =
  GetFalAiNemotronAsrStreamRequestsByRequestIdStatusResponses[keyof GetFalAiNemotronAsrStreamRequestsByRequestIdStatusResponses]

export type PutFalAiNemotronAsrStreamRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/nemotron/asr/stream/requests/{request_id}/cancel'
}

export type PutFalAiNemotronAsrStreamRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiNemotronAsrStreamRequestsByRequestIdCancelResponse =
  PutFalAiNemotronAsrStreamRequestsByRequestIdCancelResponses[keyof PutFalAiNemotronAsrStreamRequestsByRequestIdCancelResponses]

export type PostFalAiNemotronAsrStreamData = {
  body: SchemaNemotronAsrStreamInput
  path?: never
  query?: never
  url: '/fal-ai/nemotron/asr/stream'
}

export type PostFalAiNemotronAsrStreamResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiNemotronAsrStreamResponse =
  PostFalAiNemotronAsrStreamResponses[keyof PostFalAiNemotronAsrStreamResponses]

export type GetFalAiNemotronAsrStreamRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/nemotron/asr/stream/requests/{request_id}'
}

export type GetFalAiNemotronAsrStreamRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaNemotronAsrStreamOutput
}

export type GetFalAiNemotronAsrStreamRequestsByRequestIdResponse =
  GetFalAiNemotronAsrStreamRequestsByRequestIdResponses[keyof GetFalAiNemotronAsrStreamRequestsByRequestIdResponses]

export type GetFalAiNemotronAsrRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/nemotron/asr/requests/{request_id}/status'
}

export type GetFalAiNemotronAsrRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiNemotronAsrRequestsByRequestIdStatusResponse =
  GetFalAiNemotronAsrRequestsByRequestIdStatusResponses[keyof GetFalAiNemotronAsrRequestsByRequestIdStatusResponses]

export type PutFalAiNemotronAsrRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/nemotron/asr/requests/{request_id}/cancel'
}

export type PutFalAiNemotronAsrRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiNemotronAsrRequestsByRequestIdCancelResponse =
  PutFalAiNemotronAsrRequestsByRequestIdCancelResponses[keyof PutFalAiNemotronAsrRequestsByRequestIdCancelResponses]

export type PostFalAiNemotronAsrData = {
  body: SchemaNemotronAsrInput
  path?: never
  query?: never
  url: '/fal-ai/nemotron/asr'
}

export type PostFalAiNemotronAsrResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiNemotronAsrResponse =
  PostFalAiNemotronAsrResponses[keyof PostFalAiNemotronAsrResponses]

export type GetFalAiNemotronAsrRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/nemotron/asr/requests/{request_id}'
}

export type GetFalAiNemotronAsrRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaNemotronAsrOutput
}

export type GetFalAiNemotronAsrRequestsByRequestIdResponse =
  GetFalAiNemotronAsrRequestsByRequestIdResponses[keyof GetFalAiNemotronAsrRequestsByRequestIdResponses]

export type GetFalAiSileroVadRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/silero-vad/requests/{request_id}/status'
}

export type GetFalAiSileroVadRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSileroVadRequestsByRequestIdStatusResponse =
  GetFalAiSileroVadRequestsByRequestIdStatusResponses[keyof GetFalAiSileroVadRequestsByRequestIdStatusResponses]

export type PutFalAiSileroVadRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/silero-vad/requests/{request_id}/cancel'
}

export type PutFalAiSileroVadRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSileroVadRequestsByRequestIdCancelResponse =
  PutFalAiSileroVadRequestsByRequestIdCancelResponses[keyof PutFalAiSileroVadRequestsByRequestIdCancelResponses]

export type PostFalAiSileroVadData = {
  body: SchemaSileroVadInput
  path?: never
  query?: never
  url: '/fal-ai/silero-vad'
}

export type PostFalAiSileroVadResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSileroVadResponse =
  PostFalAiSileroVadResponses[keyof PostFalAiSileroVadResponses]

export type GetFalAiSileroVadRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/silero-vad/requests/{request_id}'
}

export type GetFalAiSileroVadRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSileroVadOutput
}

export type GetFalAiSileroVadRequestsByRequestIdResponse =
  GetFalAiSileroVadRequestsByRequestIdResponses[keyof GetFalAiSileroVadRequestsByRequestIdResponses]

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: {
      /**
       * Whether to include logs (`1`) in the response or not (`0`).
       */
      logs?: number
    }
    url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2/requests/{request_id}/status'
  }

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelData =
  {
    body?: never
    path: {
      /**
       * Request ID
       */
      request_id: string
    }
    query?: never
    url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2/requests/{request_id}/cancel'
  }

export type PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsSpeechToTextScribeV2Data = {
  body: SchemaElevenlabsSpeechToTextScribeV2Input
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2'
}

export type PostFalAiElevenlabsSpeechToTextScribeV2Responses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsSpeechToTextScribeV2Response =
  PostFalAiElevenlabsSpeechToTextScribeV2Responses[keyof PostFalAiElevenlabsSpeechToTextScribeV2Responses]

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/scribe-v2/requests/{request_id}'
}

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponses =
  {
    /**
     * Result of the request.
     */
    200: SchemaElevenlabsSpeechToTextScribeV2Output
  }

export type GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponse =
  GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponses[keyof GetFalAiElevenlabsSpeechToTextScribeV2RequestsByRequestIdResponses]

export type GetFalAiSmartTurnRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/smart-turn/requests/{request_id}/status'
}

export type GetFalAiSmartTurnRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSmartTurnRequestsByRequestIdStatusResponse =
  GetFalAiSmartTurnRequestsByRequestIdStatusResponses[keyof GetFalAiSmartTurnRequestsByRequestIdStatusResponses]

export type PutFalAiSmartTurnRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/smart-turn/requests/{request_id}/cancel'
}

export type PutFalAiSmartTurnRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSmartTurnRequestsByRequestIdCancelResponse =
  PutFalAiSmartTurnRequestsByRequestIdCancelResponses[keyof PutFalAiSmartTurnRequestsByRequestIdCancelResponses]

export type PostFalAiSmartTurnData = {
  body: SchemaSmartTurnInput
  path?: never
  query?: never
  url: '/fal-ai/smart-turn'
}

export type PostFalAiSmartTurnResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSmartTurnResponse =
  PostFalAiSmartTurnResponses[keyof PostFalAiSmartTurnResponses]

export type GetFalAiSmartTurnRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/smart-turn/requests/{request_id}'
}

export type GetFalAiSmartTurnRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSmartTurnOutput
}

export type GetFalAiSmartTurnRequestsByRequestIdResponse =
  GetFalAiSmartTurnRequestsByRequestIdResponses[keyof GetFalAiSmartTurnRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextTurboRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/turbo/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextTurboRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextTurboRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextTurboRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextTurboData = {
  body: SchemaSpeechToTextTurboInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text/turbo'
}

export type PostFalAiSpeechToTextTurboResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextTurboResponse =
  PostFalAiSpeechToTextTurboResponses[keyof PostFalAiSpeechToTextTurboResponses]

export type GetFalAiSpeechToTextTurboRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/requests/{request_id}'
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextTurboOutput
}

export type GetFalAiSpeechToTextTurboRequestsByRequestIdResponse =
  GetFalAiSpeechToTextTurboRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextTurboRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/turbo/stream/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextTurboStreamRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/stream/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextTurboStreamRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextTurboStreamData = {
  body: SchemaSpeechToTextTurboStreamInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text/turbo/stream'
}

export type PostFalAiSpeechToTextTurboStreamResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextTurboStreamResponse =
  PostFalAiSpeechToTextTurboStreamResponses[keyof PostFalAiSpeechToTextTurboStreamResponses]

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/turbo/stream/requests/{request_id}'
}

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextTurboStreamOutput
}

export type GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponse =
  GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextTurboStreamRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextStreamRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/stream/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextStreamRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextStreamRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/stream/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextStreamRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextStreamData = {
  body: SchemaSpeechToTextStreamInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text/stream'
}

export type PostFalAiSpeechToTextStreamResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextStreamResponse =
  PostFalAiSpeechToTextStreamResponses[keyof PostFalAiSpeechToTextStreamResponses]

export type GetFalAiSpeechToTextStreamRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/stream/requests/{request_id}'
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextStreamOutput
}

export type GetFalAiSpeechToTextStreamRequestsByRequestIdResponse =
  GetFalAiSpeechToTextStreamRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextStreamRequestsByRequestIdResponses]

export type GetFalAiSpeechToTextRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/speech-to-text/requests/{request_id}/status'
}

export type GetFalAiSpeechToTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiSpeechToTextRequestsByRequestIdStatusResponse =
  GetFalAiSpeechToTextRequestsByRequestIdStatusResponses[keyof GetFalAiSpeechToTextRequestsByRequestIdStatusResponses]

export type PutFalAiSpeechToTextRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/requests/{request_id}/cancel'
}

export type PutFalAiSpeechToTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiSpeechToTextRequestsByRequestIdCancelResponse =
  PutFalAiSpeechToTextRequestsByRequestIdCancelResponses[keyof PutFalAiSpeechToTextRequestsByRequestIdCancelResponses]

export type PostFalAiSpeechToTextData = {
  body: SchemaSpeechToTextInput
  path?: never
  query?: never
  url: '/fal-ai/speech-to-text'
}

export type PostFalAiSpeechToTextResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiSpeechToTextResponse =
  PostFalAiSpeechToTextResponses[keyof PostFalAiSpeechToTextResponses]

export type GetFalAiSpeechToTextRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/speech-to-text/requests/{request_id}'
}

export type GetFalAiSpeechToTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaSpeechToTextOutput
}

export type GetFalAiSpeechToTextRequestsByRequestIdResponse =
  GetFalAiSpeechToTextRequestsByRequestIdResponses[keyof GetFalAiSpeechToTextRequestsByRequestIdResponses]

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/elevenlabs/speech-to-text/requests/{request_id}/status'
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponse =
  GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponses[keyof GetFalAiElevenlabsSpeechToTextRequestsByRequestIdStatusResponses]

export type PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/requests/{request_id}/cancel'
}

export type PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponse =
  PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponses[keyof PutFalAiElevenlabsSpeechToTextRequestsByRequestIdCancelResponses]

export type PostFalAiElevenlabsSpeechToTextData = {
  body: SchemaElevenlabsSpeechToTextInput
  path?: never
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text'
}

export type PostFalAiElevenlabsSpeechToTextResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiElevenlabsSpeechToTextResponse =
  PostFalAiElevenlabsSpeechToTextResponses[keyof PostFalAiElevenlabsSpeechToTextResponses]

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/elevenlabs/speech-to-text/requests/{request_id}'
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaElevenlabsSpeechToTextOutput
}

export type GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponse =
  GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponses[keyof GetFalAiElevenlabsSpeechToTextRequestsByRequestIdResponses]

export type GetFalAiWizperRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/wizper/requests/{request_id}/status'
}

export type GetFalAiWizperRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWizperRequestsByRequestIdStatusResponse =
  GetFalAiWizperRequestsByRequestIdStatusResponses[keyof GetFalAiWizperRequestsByRequestIdStatusResponses]

export type PutFalAiWizperRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wizper/requests/{request_id}/cancel'
}

export type PutFalAiWizperRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWizperRequestsByRequestIdCancelResponse =
  PutFalAiWizperRequestsByRequestIdCancelResponses[keyof PutFalAiWizperRequestsByRequestIdCancelResponses]

export type PostFalAiWizperData = {
  body: SchemaWizperInput
  path?: never
  query?: never
  url: '/fal-ai/wizper'
}

export type PostFalAiWizperResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWizperResponse =
  PostFalAiWizperResponses[keyof PostFalAiWizperResponses]

export type GetFalAiWizperRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/wizper/requests/{request_id}'
}

export type GetFalAiWizperRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWizperOutput
}

export type GetFalAiWizperRequestsByRequestIdResponse =
  GetFalAiWizperRequestsByRequestIdResponses[keyof GetFalAiWizperRequestsByRequestIdResponses]

export type GetFalAiWhisperRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/fal-ai/whisper/requests/{request_id}/status'
}

export type GetFalAiWhisperRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetFalAiWhisperRequestsByRequestIdStatusResponse =
  GetFalAiWhisperRequestsByRequestIdStatusResponses[keyof GetFalAiWhisperRequestsByRequestIdStatusResponses]

export type PutFalAiWhisperRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/whisper/requests/{request_id}/cancel'
}

export type PutFalAiWhisperRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutFalAiWhisperRequestsByRequestIdCancelResponse =
  PutFalAiWhisperRequestsByRequestIdCancelResponses[keyof PutFalAiWhisperRequestsByRequestIdCancelResponses]

export type PostFalAiWhisperData = {
  body: SchemaWhisperInput
  path?: never
  query?: never
  url: '/fal-ai/whisper'
}

export type PostFalAiWhisperResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostFalAiWhisperResponse =
  PostFalAiWhisperResponses[keyof PostFalAiWhisperResponses]

export type GetFalAiWhisperRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/fal-ai/whisper/requests/{request_id}'
}

export type GetFalAiWhisperRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaWhisperOutput
}

export type GetFalAiWhisperRequestsByRequestIdResponse =
  GetFalAiWhisperRequestsByRequestIdResponses[keyof GetFalAiWhisperRequestsByRequestIdResponses]

export type GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/half-moon-ai/ai-detector/detect-text/requests/{request_id}/status'
}

export type GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdStatusResponse =
  GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdStatusResponses[keyof GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdStatusResponses]

export type PutHalfMoonAiAiDetectorDetectTextRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/half-moon-ai/ai-detector/detect-text/requests/{request_id}/cancel'
}

export type PutHalfMoonAiAiDetectorDetectTextRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutHalfMoonAiAiDetectorDetectTextRequestsByRequestIdCancelResponse =
  PutHalfMoonAiAiDetectorDetectTextRequestsByRequestIdCancelResponses[keyof PutHalfMoonAiAiDetectorDetectTextRequestsByRequestIdCancelResponses]

export type PostHalfMoonAiAiDetectorDetectTextData = {
  body: SchemaAiDetectorDetectTextInput
  path?: never
  query?: never
  url: '/half-moon-ai/ai-detector/detect-text'
}

export type PostHalfMoonAiAiDetectorDetectTextResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostHalfMoonAiAiDetectorDetectTextResponse =
  PostHalfMoonAiAiDetectorDetectTextResponses[keyof PostHalfMoonAiAiDetectorDetectTextResponses]

export type GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/half-moon-ai/ai-detector/detect-text/requests/{request_id}'
}

export type GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaAiDetectorDetectTextOutput
}

export type GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdResponse =
  GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdResponses[keyof GetHalfMoonAiAiDetectorDetectTextRequestsByRequestIdResponses]

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/openrouter/router/video/enterprise/requests/{request_id}/status'
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponses =
  {
    /**
     * The request status.
     */
    200: SchemaQueueStatus
  }

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdStatusResponses]

export type PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/enterprise/requests/{request_id}/cancel'
}

export type PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponses =
  {
    /**
     * The request was cancelled.
     */
    200: {
      /**
       * Whether the request was cancelled successfully.
       */
      success?: boolean
    }
  }

export type PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterVideoEnterpriseRequestsByRequestIdCancelResponses]

export type PostOpenrouterRouterVideoEnterpriseData = {
  body: SchemaRouterVideoEnterpriseInput
  path?: never
  query?: never
  url: '/openrouter/router/video/enterprise'
}

export type PostOpenrouterRouterVideoEnterpriseResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostOpenrouterRouterVideoEnterpriseResponse =
  PostOpenrouterRouterVideoEnterpriseResponses[keyof PostOpenrouterRouterVideoEnterpriseResponses]

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/enterprise/requests/{request_id}'
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaRouterVideoEnterpriseOutput
}

export type GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponse =
  GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponses[keyof GetOpenrouterRouterVideoEnterpriseRequestsByRequestIdResponses]

export type GetOpenrouterRouterVideoRequestsByRequestIdStatusData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: {
    /**
     * Whether to include logs (`1`) in the response or not (`0`).
     */
    logs?: number
  }
  url: '/openrouter/router/video/requests/{request_id}/status'
}

export type GetOpenrouterRouterVideoRequestsByRequestIdStatusResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type GetOpenrouterRouterVideoRequestsByRequestIdStatusResponse =
  GetOpenrouterRouterVideoRequestsByRequestIdStatusResponses[keyof GetOpenrouterRouterVideoRequestsByRequestIdStatusResponses]

export type PutOpenrouterRouterVideoRequestsByRequestIdCancelData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/requests/{request_id}/cancel'
}

export type PutOpenrouterRouterVideoRequestsByRequestIdCancelResponses = {
  /**
   * The request was cancelled.
   */
  200: {
    /**
     * Whether the request was cancelled successfully.
     */
    success?: boolean
  }
}

export type PutOpenrouterRouterVideoRequestsByRequestIdCancelResponse =
  PutOpenrouterRouterVideoRequestsByRequestIdCancelResponses[keyof PutOpenrouterRouterVideoRequestsByRequestIdCancelResponses]

export type PostOpenrouterRouterVideoData = {
  body: SchemaRouterVideoInput
  path?: never
  query?: never
  url: '/openrouter/router/video'
}

export type PostOpenrouterRouterVideoResponses = {
  /**
   * The request status.
   */
  200: SchemaQueueStatus
}

export type PostOpenrouterRouterVideoResponse =
  PostOpenrouterRouterVideoResponses[keyof PostOpenrouterRouterVideoResponses]

export type GetOpenrouterRouterVideoRequestsByRequestIdData = {
  body?: never
  path: {
    /**
     * Request ID
     */
    request_id: string
  }
  query?: never
  url: '/openrouter/router/video/requests/{request_id}'
}

export type GetOpenrouterRouterVideoRequestsByRequestIdResponses = {
  /**
   * Result of the request.
   */
  200: SchemaRouterVideoOutput
}

export type GetOpenrouterRouterVideoRequestsByRequestIdResponse =
  GetOpenrouterRouterVideoRequestsByRequestIdResponses[keyof GetOpenrouterRouterVideoRequestsByRequestIdResponses]
